---
layout: base
title: AI Pedagogy
author: Fred Gibbs
date: 2025-02-14
description: "Amaranth investigates AI pedagogy from a humanistic perspective: how undergraduate students develop critical AI fluency through open workflows, public scholarship, and interpretive practice at UNM."
header-image: /assets/images/headers/mechanical-turk.webp
header-title: AI Pedagogy
header-tier: section
header-filter: photo
header-zoom: 110%
header-position: center 20%
---

## Head in sand ≠ good AI policy
Artificial intelligence is reshaping how knowledge is made, organized, and interpreted. While it may be tempting to flee the onslaught of Gen AI agents, humanists must to be invovled. The debates AI generates—about authorship, bias, authority, expertise, and trust—are debates humanists have been having for decades. What makes a source reliable? Who decides what counts as knowledge? How do systems of power shape what gets preserved and what gets discarded?

It's common to feel helpless as tech giants pump out AI products out by the week. Of course these exert some influence, but we're not helpless. After all, they need to sell stuff. We want humanists helping to decide what tools and processes and values rise to the surface. The question isn't whether to engage with AI, but how we can get everyone asking better questions. 


## Digital work as introduction to AI
For years, technical barriers have kept humanists from building the kinds of digital projects their scholarship deserved. Anything more than a Wordpress site full of ads was basically out of bounds, much less a design-forward digital exhibit, or a collaborative oral history project. Most scholars never cleared that threshold.

AI changes what's possible. When working with clearly structured, well-documented systems—like Amaranth's [Xanthan](https://xanthan-web.github.io) framework—AI assistants can handle the technical details while you focus on intellectual decisions. You don't need to master technical details to adjust a site's typography. You don't need to understand databases to organize your images. The technical code is still there (and adjustable once you learn more), but it's no longer a prerequisite for getting started.

This isn't about replacing or outsourcing expertise. It's about letting people with humanistic vision shape their own work and expand their technical skills without spending months learning infrastructure first. And in the process of building something real---a website, a digital project, a public-facing piece of scholarship---you start to understand how to push boundaries deliberately.


## A gateway to broader AI engagement
We've found that working with AI on **technical tasks**—changing a website layout, formatting a document, troubleshooting code—offers a lower-stakes entry point than asking AI to analyze a historical text or draft an argument. Technical work has clearer success criteria. You can see whether the font is bigger, whether the colors go together, whether the site builds correctly. There's less at stake intellectually, which makes it easier to experiment.

This matters because many faculty and students approach AI with justified caution—concerns about plagiarism, intellectual shortcuts, or undermining the learning process. By starting with low-level technical support for digital humanities work, we (respectfully) sidestep some of those important concerns. Once people get comfortable using AI as a technical collaborator, they're better positioned to think critically about how it might (or might not) play a role in their scholarship and teaching.

Digital humanities projects become a training ground for AI fluency more broadly.


## We learn alongside you
We don't pretend we're AI experts who have it figured out. We're practitioners learning on the fly, constantly using AI for technical work, design experiments, troubleshooting, documentation. We're  constantly learning where it helps and where it misleads, both in technical and humanistic contexts.

When we work with collaborators, we share what we're learning: A particular prompt iteration. When and how to ask for explanation. Here's where AI saved us hours, and here's where it sent us into the weeds. None of us have mastered this yet, but we're all better working and sharing together.


## Part of a new digital humanities moment
This approach to AI is central to Amaranth's broader digital humanities work. As we discuss on the [Digital Humanities](/studio/digital-humanities) page, AI has dramatically lowered the barrier to entry for digital scholarship. You no longer need to learn a programming language to build something compelling or do sophisticated text analysis. You need good questions, editorial judgment, and the willingness to learn through making.

That shift opens up digital humanities to a much wider community of practitioners. It also demands a different kind of pedagogy—one that teaches critical collaboration with AI systems rather than technical mastery as a prerequisite.


## An evolving practice
Our work with AI isn't settled. We're learning semester by semester, project by project, constantly adjusting what we teach and how we guide people through these workflows. We document what we're discovering—what works, what doesn't, where the boundaries are—and share it as we go.

If you're working with AI in your teaching, your research, or your digital projects and want to compare notes, [reach out](/collaborate). We all need to figure this out together.
